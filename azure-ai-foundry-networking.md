# Azure AI Foundry Networking and Security at Enterprise Scale

## Overview

Azure AI Foundry (formerly Azure AI Studio) is a unified, enterprise-ready platform for building, testing, deploying, and managing generative AI applications at scale. It provides a web portal and integrated toolset that let AI developers and data scientists collaborate on AI solutions with enterprise-grade governance and security in place. By consolidating Azure’s AI services (like Azure OpenAI, Cognitive Services, and Search) with compute infrastructure and tools, AI Foundry accelerates AI development while ensuring compliance, security, and scalability for production use.

At a high level, Azure AI Foundry offers a Management Center to govern resources and security, a top-level Hub to define shared configurations and resources, and multiple Projects under each Hub to isolate workloads. This structure allows IT teams to set up a preconfigured AI environment (the Hub) that developers can use to spin up projects quickly without waiting on bespoke infrastructure setups. In practice, this means organizations can prototype and implement AI solutions faster, with the Hub enforcing central policies (networking, identity, encryption, etc.) across all projects automatically.

## Hub and Projects

### Azure AI Foundry Hub

The hub is the top-level resource container and is based on the Azure Machine Learning workspace service (with a special kind Hub). It acts as a central management plane for a team or department’s AI environment. The hub defines common settings and shared assets, including: network and security configurations (like a managed virtual network spanning its projects and endpoints), allocated compute capacity and quotas, connections to external data or AI services, and an associated storage account for data and artifacts. A hub can host multiple projects, and any security or networking setting on the hub is inherited by its projects (enforcing uniform guardrails). For example, administrators can configure the hub with a private network and custom encryption, and all child projects will automatically use those settings. The hub also manages shared resources like foundational AI model endpoints (Azure OpenAI, etc.) that can be accessed across projects via Connections (authenticated links to external resources). In essence, the hub is where IT admins or platform engineers configure the “environment” – virtual network, identity integration, keys, policies, and so on – and manage high-level resources and user access.

### Azure AI Foundry Project

A project is an isolated workspace within a hub, intended for an individual team, application, or AI workload. Technically, projects are also Azure ML workspaces (kind Project) but they inherit the hub’s network, identity, and baseline security settings. Each project serves as a sandbox for developers or data scientists to build and deploy AI solutions without affecting other projects. Projects isolate assets like data, models, and experiments: for example, each project gets its own section of the hub’s storage (a dedicated container or path) for datasets and outputs. Project members can create their own compute instances, fine-tune or import models, and deploy endpoints, all within the resource and policy boundaries defined by the hub. Projects can also have project-scoped connections to data sources or services that shouldn’t be shared with other projects – for instance, a project might connect to a confidential storage account that only that team should use. While projects inherit the hub’s configurations, they allow further role-based access control at the project level (so a user could be a Contributor in one project but have no access to another project in the same hub). In summary, if the hub is analogous to a virtual data center for AI, then projects are like the individual applications or workloads running within it, each with separation of data and permissions but all governed by the hub’s overarching security and connectivity rules.

## AI Foundry Networking 

Networking is a critical aspect of Azure AI Foundry’s security model, ensuring that compute resources and data remain isolated from unwanted network access. Azure AI Foundry provides robust network isolation options for both hub and project resources, particularly for managed compute instances (interactive development VMs), training clusters, and deployed model endpoints (including serverless and managed online endpoints). At a high level, there are controls for both outbound and inbound network access: you can restrict outbound internet connectivity from compute resources using a managed virtual network, and you can secure inbound access to the hub and its services using Private Link (private endpoints) and other networking features. This section describes the networking models and isolation patterns, including the managed virtual network modes, private endpoint configuration, and how to enable on-premises connectivity for hybrid scenarios.

### Managed Virtual Network and Outbound Isolation Modes

When you create an Azure AI Foundry hub, you have the option to enable a [managed virtual network](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/configure-managed-network?tabs=portal) for all the hub’s compute resources.

![Network Isolation Modes](images/NetworkIsolationModes.png)

This managed VNet is automatically used by any projects under the hub, providing a central egress control point for all project compute. Azure AI Foundry offers three modes of network isolation for outbound traffic from this managed virtual network:

- **Allow Internet Outbound**: All outbound traffic from managed compute is allowed to go directly to the internet (no restrictions on egress). This mode is suitable for less restrictive environments or early development, as it permits VM instances or containers to download updates, Python packages, and communicate with external endpoints freely (useful for quickly getting started). It still supports using Private Link for Azure services if desired, but by default all internet destinations are reachable.
- **Allow Only Approved Outbound**: Outbound traffic from the managed network is denied by default, except for traffic to specific approved targets. In this mode, you explicitly define what external services or endpoints the compute resources can reach. Azure AI Foundry preconfigures required rules for core Azure services so that the workspace can function (for example, allowing traffic to Azure management endpoints and to necessary services like Azure Container Registry). You can then add your own outbound rules (using Azure service tags, specific FQDNs, or IP ranges) to permit access to approved external resources. This mode greatly reduces the risk of data exfiltration – compute instances cannot call out to arbitrary internet sites, only those on the safe list. (Note that using this mode may require preparing all needed packages or data in approved locations. Any attempt to reach non-approved endpoints will be blocked.) By default, in the “Allow Only Approved” mode, Azure AI Foundry will route traffic to its own dependent Azure resources (like the hub’s storage account, key vault, and container registry) through private endpoints, since direct internet access is restricted. This ensures that even communications with Azure’s back-end services occur over private Azure network channels.
- **Disabled (No Managed Network)**: In this context, “Disabled” means the managed virtual network isolation is turned off for the hub’s workspace. Compute resources are not placed in an Azure AI Foundry-managed VNet for egress; instead they either use the public network or a customer-provided VNet configuration. In other words, there are no automatic restrictions on inbound or outbound traffic at the workspace level. This mode might be chosen if you plan to implement network isolation yourself (for example, attaching compute to your own VNet with custom NSG rules), or if you require public IP access. However, using no managed network means outbound traffic is essentially unrestricted by the Foundry platform (all egress is allowed), and it relies on either the default Azure infrastructure security or any custom network you integrate. (In Azure AI Foundry, if you start with the managed network disabled, you cannot later switch the workspace to use the managed network without redeploying; similarly, once a managed network is enabled with a certain mode, there are limitations on changing it.)

### Private Network Access (Inbound) and Private Links

In addition to controlling outbound connectivity, Azure AI Foundry allows you to lock down inbound network access to your hubs and projects. By default, a hub (being an Azure ML workspace) might be accessible over the public internet (through Azure’s endpoints) if you have the right credentials. For stricter security, you can disable public network access at the hub level and use [Azure Private Link](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/configure-private-link?tabs=azure-portal) to expose the hub and its studio interface only within your private network. A private endpoint is essentially a NIC with a private IP in your VNet that is linked to the Azure service – in this case, the Azure AI Foundry hub. When a private endpoint is set up, all traffic to the hub’s API (and UI) can be forced through your VNet; no traffic from the service uses a public IP. Enterprise customers often require this configuration to meet internal network security policies, ensuring that even the portal and APIs are only reachable from within approved network boundaries.

To secure inbound access to an AI Foundry environment, Microsoft’s guidance is to disable the public network access flag on all relevant resources and use private endpoints instead. In the Azure portal, for a given hub, you can navigate to Networking settings and set Public network access to “Disabled”, then create a private endpoint connection for the hub. Likewise, any connected Azure AI services (like the Azure OpenAI service resource, the multi-service Azure AI resource, or Azure Cognitive Search index) can have their public access disabled and private endpoints created. The same goes for the default storage account, key vault, and container registry that support the hub – by enabling private endpoints for these, you ensure that data and model artifacts are only accessible within your private network. This effectively ring-fences the entire AI Foundry deployment. The AI Foundry portal supports this configuration by allowing the studio UI and APIs to be accessed via the private endpoint.

With public access turned off, all clients (including developers’ browsers or SDKs) must connect via the private endpoint – which usually means you need connectivity from your on-premises or development environment into the Azure VNet (e.g., via VPN or ExpressRoute) to reach the hub. Internally, the hub’s URL (for example, a workspace ID URL) will resolve to a private IP address. DNS setup is a part of Private Link configuration (typically, Azure will create a Private DNS zone for the service, mapping the service’s public DNS name to the private IP in your VNet). Once configured, the hub and project studio, REST APIs, and other operations are locked into the private network.

It’s important to note that Azure AI Foundry’s managed virtual network and private endpoint settings apply to the platform’s managed compute and services, but do not extend to certain external services in the sense of network routing. For example, if your project uses Azure OpenAI or Azure Cognitive Services via a connection, those services are platform as a service (PaaS) endpoints that run in Microsoft’s global infrastructure. They don’t live inside your hub’s VNet, and thus the managed VNet settings don’t directly control their network access. However, using Private Links, you can secure those services as well by creating private endpoints for your Azure OpenAI resource or Cognitive Service resource – thereby ensuring calls to them go through your VNet. In short, you would disable public access on each such resource and create the appropriate private endpoints. Microsoft notes that even without private links, communication to these PaaS services from your compute is encrypted (HTTPS) and the services are multi-tenant and managed by Azure (so data exfiltration risk is mitigated by the service’s design). But for maximum security, enterprises often choose to integrate everything via private networking.

In practice, a fully isolated Azure AI Foundry deployment might have no public IP exposure at all: the hub, its storage, key vault, etc., and any connected AI services all use private endpoints. Users connect from on-prem or secure clients through an Azure VPN or ExpressRoute into the VNet to work with the Foundry portal. This setup greatly reduces the attack surface and prevents any accidental exposure of sensitive data or models to the internet.

### Hybrid Connectivity to On-Premises Resources

What if your AI project needs to access data or APIs that reside on-premises (within your corporate network) or in a different Azure VNet? Azure AI Foundry supports this via a pattern using [Azure Application Gateway and Private Link](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/access-on-premises-resources) to bridge the managed workspace network with on-premises resources. According to Microsoft’s documentation, to reach a non-Azure or on-prem resource from the Foundry’s managed VNet, you should deploy an Application Gateway in your Azure environment (in a subnet that has line-of-sight to the on-prem resource, e.g. via ExpressRoute) and then create a private endpoint from the AI Foundry managed VNet to that Application Gateway. The Application Gateway acts as a proxy that terminates and routes requests from your AI Foundry compute to the on-prem system. By using a private endpoint connection to the gateway, the traffic from Foundry’s perspective stays within a private Azure network path and then the gateway can forward it to the on-premises backend. This provides a secure, encrypted path that does not traverse the public internet at all. Essentially, the flow is: Azure AI Foundry compute (managed VNet) → Private Endpoint → Application Gateway in your VNet → on-premises resource. The Application Gateway can enforce HTTPS and other security filters, and because it’s a fully managed service, it fits well into an enterprise scenario.

For example, suppose a project needs to query an internal REST API or database that lives in a data center. You would set up an Application Gateway in Azure (same region as your Foundry hub’s VNet) with backend targets pointing to that on-prem API (through VPN/ExpressRoute). Then you’d create a Private Link for the Foundry hub (or project) to talk to the Application Gateway’s listener. The result is that your AI code running in Foundry can call the on-prem API via a private IP address (the private endpoint address), and the request travels securely to the App Gateway and onward to on-prem. Microsoft supports this configuration and has validated certain on-prem resources (e.g., JFrog Artifactory, Snowflake, etc.) working through Application Gateway. This approach allows you to integrate on-prem data sources into your AI workflows without opening up public access or breaking the network isolation. Do note that setting this up requires coordination – you must deploy and configure the Application Gateway and ensure your on-prem network is reachable from it (typically via ExpressRoute or Site-to-Site VPN), and then configure the private endpoint. But once in place, it provides end-to-end private connectivity for hybrid AI scenarios.

## Identity and Access Management

Azure AI Foundry builds on Azure’s identity and access management framework (Azure AD/Microsoft Entra ID) to secure who can access the hub and projects, and what operations they can perform. Authentication is handled via Azure AD – users log in to the AI Foundry portal with their Entra ID credentials. Authorization is enforced through Azure Role-Based Access Control (RBAC) at two levels: the hub level and the project level.

At the Hub scope, an administrator can assign users or groups one of several built-in roles that determine their permissions across the hub and all its projects. The default hub roles include: Owner, Contributor, Azure AI Developer, Azure AI Inference Deployment Operator, and Reader, among a couple of special-purpose roles. An Owner of a hub has full control, including managing security and creating or deleting the hub itself. A Contributor can do almost everything in the hub (like create projects and resources) but cannot manage the hub’s own access permissions. The Azure AI Developer role is a hub-scoped role intended for team members who need to use the hub and create projects but should not create new hubs or touch hub-level settings. This role can create projects under the hub, use compute, deploy models, and manage assets, but cannot modify hub security. (In practice, if you want to prevent users from accidentally creating new separate hubs – which might incur costs or sprawl – you’d give them Azure AI Developer instead of Contributor.) The Azure AI Inference Deployment Operator is a more limited role focused on deploying AI services – for instance, someone who manages model endpoint deployments but not other aspects. Finally, Reader provides read-only view access. Notably, when someone is added as a member of any project in the hub, they are automatically granted Reader access at the hub level as well (this is so they can at least see the hub context). Only Owners or Contributors (or a custom role with similar rights) can on-board new users to a hub or assign roles.

At the Project scope, there is a similar RBAC model, typically with Project Contributor and Project Reader roles (and the Inference Deployment Operator applies here as well for allowing deployments in the resource group). A Project Contributor can use and manage all resources within that project (create notebooks, pipelines, deploy models, etc.) but cannot affect other projects and cannot change the project’s own access control (unless explicitly given permission). Project Readers can view assets but not make changes. There isn’t a “Project Owner” separate from the hub roles; effectively, if you have permission to create a project (via your hub role), you’ll likely be a Project Contributor in it. In Azure AI Foundry, adding a user to a project through the portal will behind the scenes assign them the appropriate Azure ML workspace role as well as ensure they have minimal access to the hub and underlying resource group to function. The platform tries to simplify this: for example, if you invite a user to a project as a Contributor, the system will automatically give them Reader on the hub plus an “Inference Deployment Operator” on the resource group so they can deploy endpoints.

It’s worth mentioning that role inheritance flows from hub to projects: if you are a Hub Owner, you inherently can access all projects in that hub (with full control). But you could be a Contributor or Reader on just a specific project and have no rights on other projects. This flexibility allows multi-tenancy within a single hub – multiple teams can share the hub’s infrastructure but only see their own projects.